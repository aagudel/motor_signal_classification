{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Offline decoding\n",
    "\n",
    "In this notebook, I compare the performance that multiple decoders achieve on the same test set. Every test will be repeated K (standard = 10) times changing the training set at every iteration. The K training sets will be the ones also used to tune the hyperparameters. That step is described in the notebook: [decoders_hyperparameters_optimization](./decoders_hyperparameters_optimization.ipynb).\n",
    " - The first few sections (1-3) just import packages, load the files, and preprocess them\n",
    " - Section 4 test the performance on the test set of the 5 decoders: DNN, RNN, GRU, LSTM, CNN, EEGNet\n",
    " - Section 5 display plots to compare the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages\n",
    "\n",
    "Below, we import both standard packages, and functions from the accompanying .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding working directory to python path\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "np.random.seed(27) # Seed is important for having the same test set for performance comparison\n",
    "import pickle as pkl\n",
    "import json\n",
    "from utils.functions import *\n",
    "from math import ceil\n",
    "\n",
    "#Import tensorflow\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#Import graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#Import function to laod and process dataset\n",
    "from utils.data_processing import * \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Import metrics\n",
    "from sklearn.metrics import accuracy_score, r2_score, log_loss\n",
    "\n",
    "#Import decoder functions\n",
    "from utils.decoders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn off deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Neural data collected as numpy array of windows \"number of neurons\" x \"number of time bins\", where each entry is the firing rate of a given neuron in a given time bin. To each window is associated the object grasped in that trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. User Inputs\n",
    "It is possible to define the file (i.e. the monkey) and the epoch to use as input of the decoder. Moreover, using the class ObjectSelector it is possible to define which objects to include and whether to group all the different sizes in a single class. K is the number of repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'MRec40'  # MRec40, ZRec50 or ZRec50_Mini\n",
    "PATH = f'../data/Objects Task DL Project/{FILE}.neo.mat' \n",
    "EPOCH = 'cue'\n",
    "K = 5\n",
    "\n",
    "selector = ObjectSelector()\n",
    "new_classes = selector.get_non_special(group_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements, objects = load_dataset(PATH, EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data\n",
    "It is possible to define wheter to apply one-hot encoding or to normalize the number of elements for each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape and Y shape\n",
      "(432, 552, 10) (432, 36)\n",
      "object ID and repetition in the dataset:\n",
      "('21', 12)\t('22', 12)\t('23', 12)\t('24', 12)\t('25', 12)\t('26', 12)\t('31', 12)\t('32', 12)\t('33', 12)\t('34', 12)\t('35', 12)\t('36', 12)\t('41', 12)\t('42', 12)\t('43', 12)\t('44', 12)\t('45', 12)\t('46', 12)\t('51', 12)\t('52', 12)\t('53', 12)\t('54', 12)\t('55', 12)\t('56', 12)\t('61', 12)\t('62', 12)\t('63', 12)\t('64', 12)\t('65', 12)\t('66', 12)\t('71', 12)\t('72', 12)\t('73', 12)\t('74', 12)\t('75', 12)\t('76', 12)\t"
     ]
    }
   ],
   "source": [
    "# Preprocessing measurements\n",
    "label_encoder = LabelEncoder()\n",
    "X, Y = preprocess_dataset(measurements, objects, labelled_classes=new_classes, one_hot_encoder=label_encoder, norm_classes=False)\n",
    "\n",
    "outputs = Y.shape[1]\n",
    "(_, channels, window) = X.shape\n",
    "outputs = Y.shape[1]\n",
    "\n",
    "print('X shape and Y shape')\n",
    "print(X.shape, Y.shape)\n",
    "unique_y, n_repetition = np.unique(label_encoder.inverse_transform(Y.argmax(axis=1)), return_counts=True, axis=0)\n",
    "print('object ID and repetition in the dataset:' )\n",
    "for elem in zip(unique_y, n_repetition):\n",
    "    print(elem, end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Split into training / testing / validation sets\n",
    "Note that hyperparameters should be determined using a separate validation set. \n",
    "Then, the goodness of fit should be be tested on a testing set (separate from the training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of repetitions K: 5\n",
      "train: 5*(302, 552, 10) -- validation: 5*(65, 552, 10) -- test: (65, 552, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = split_sets(X, Y, tr_split=0.7, val_split=0.15, repetitions=K)\n",
    "print(f'number of repetitions K: {K}')\n",
    "print(f'train: {K}*{np.array(x_train).shape[1:]} -- validation: {K}*{np.array(x_val).shape[1:]} -- test: {np.array(x_test).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimize Hyperparameters of decoders using \"Hyperopt\"\n",
    "The general idea is that we will try to find the decoder hyperparameters that produce the highest accuracy score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(n_samples, n_channels, n_outputs, decoder_name, config_dict):\n",
    "    (my_network, my_params) = (None, None)\n",
    "\n",
    "    if decoder_name == 'DenseNN':\n",
    "        my_params = config_dict['dnn']\n",
    "        my_network = DenseNNClassification(n_samples, n_channels, n_outputs,\n",
    "                                           units=int(my_params['num_units']),\n",
    "                                           dropout=my_params['frac_dropout'])\n",
    "\n",
    "    if decoder_name == 'SimpleRNN':\n",
    "        my_params = config_dict['rnn']\n",
    "        my_network = SimpleRNNClassification(n_samples, n_channels, n_outputs,\n",
    "                                             units=int(my_params['num_units']),\n",
    "                                             dropout=my_params['frac_dropout'])\n",
    "\n",
    "    if decoder_name == 'GRU':\n",
    "        my_params = config_dict['gru']\n",
    "        my_network = GRUClassification(n_samples, n_channels, n_outputs,\n",
    "                                       units=int(my_params['num_units']),\n",
    "                                       dropout=my_params['frac_dropout'])\n",
    "\n",
    "    if decoder_name == 'LSTM':\n",
    "        my_params = config_dict['lstm']\n",
    "        my_network = LSTMClassification(n_samples, n_channels, n_outputs,\n",
    "                                        units=int(my_params['num_units']),\n",
    "                                        dropout=my_params['frac_dropout'])\n",
    "\n",
    "    if decoder_name == 'CNN':\n",
    "        my_params = config_dict['cnn']\n",
    "        my_network = CNNClassification(n_samples, n_channels, n_outputs,\n",
    "                                       filters=int(my_params['num_filters']),\n",
    "                                       size=(int(my_params['kernel_size_1']),\n",
    "                                            int(my_params['kernel_size_2'])),\n",
    "                                       dropout=my_params['frac_dropout'],\n",
    "                                       pool_size=2)\n",
    "\n",
    "    if decoder_name == 'EEGNet':\n",
    "        # my_params = config_dict['eeg_net']\n",
    "        my_params = {'fac_dropout': 0.1, 'n_epochs': 10, 'batch_size': 6}\n",
    "        my_network = EEGNet(dropout=my_params['fac_dropout'])\n",
    "\n",
    "    if decoder_name == 'EEGNetv2':\n",
    "        my_params = config_dict['eegnet2']\n",
    "        my_network = EEGNetv2(n_channels, n_outputs,\n",
    "                              filters=[my_params['n_filters_1'], my_params['n_filters_2'], my_params['n_filters_3']],\n",
    "                              filters_size=[int(my_params['size_1']), int(my_params['size_2']), int(my_params['size_3'])],\n",
    "                              dropout=my_params['frac_dropout'],\n",
    "                              units=int(my_params['n_units']),\n",
    "                              neurons=my_params['n_neurons'])\n",
    "\n",
    "    return my_network, my_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks_to_try = ['DenseNN', 'SimpleRNN', 'GRU', 'LSTM', 'CNN', 'EEGNetv2']\n",
    "with open('../utils/hyperparameters.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DenseNN ###\n",
      "Repetition: \t1/5 [0.215]\t2/5 [0.169]\t3/5 [0.185]\t4/5 [0.185]\t5/5 [0.185]\t\n",
      "Accuracy [mean | std] : 0.18769230769230769 | 0.015073783032511869\n",
      "Loss [mean | std] : 2.346352003904489 | 0.04533938683421757\n",
      "\n",
      "### SimpleRNN ###\n",
      "Repetition: \t1/5 [0.123]\t2/5 [0.108]\t3/5 [0.138]\t4/5 [0.185]\t5/5 [0.138]\t\n",
      "Accuracy [mean | std] : 0.13846153846153847 | 0.02574338543181771\n",
      "Loss [mean | std] : 2.476368814615103 | 0.05254124487756506\n",
      "\n",
      "### GRU ###\n",
      "Repetition: \t1/5 [0.231]\t2/5 [0.231]\t3/5 [0.154]\t4/5 [0.292]\t5/5 [0.292]\t\n",
      "Accuracy [mean | std] : 0.24000000000000005 | 0.05111768531026508\n",
      "Loss [mean | std] : 2.0739410378382757 | 0.057762445793382786\n",
      "\n",
      "### LSTM ###\n",
      "Repetition: \t"
     ]
    }
   ],
   "source": [
    "for net_name in networks_to_try:\n",
    "    print(f'### {net_name} ###')\n",
    "    network, params = get_decoder(window, channels, outputs, net_name, config)\n",
    "    #network.model.summary()\n",
    "\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    print('Repetition: ', end='\\t')\n",
    "    for k in range(K):\n",
    "        network.reset_weights()\n",
    "        network.fit(x_train[k], y_train[k], num_epochs=int(params['n_epochs']),\n",
    "                    batch_size=int(params['batch_size']))\n",
    "        prediction = network.predict(x_test)\n",
    "        test_accuracy.append(accuracy_score(y_true=y_test.argmax(axis=1), y_pred=prediction.argmax(axis=1)))\n",
    "        test_loss.append(log_loss(y_test, prediction))\n",
    "        print(f'{k + 1}/{K} [{round(test_accuracy[-1], 3)}]', end='\\t')\n",
    "    test_accuracy = np.array(test_accuracy)\n",
    "    test_loss = np.array(test_loss)\n",
    "    print(f'\\nAccuracy [mean | std] : {test_accuracy.mean()} | {test_accuracy.std()}')\n",
    "    print(f'Loss [mean | std] : {test_loss.mean()} | {test_loss.std()}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
